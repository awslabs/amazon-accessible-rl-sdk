{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYO Planner\n",
    "\n",
    "This example shows you how to **Bring Your Own** (BYO) planner to work with the `a2rl.Simulator` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import my_nb_path  # isort: skip\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import a2rl as wi\n",
    "from a2rl.nbtools import print  # Enable color outputs when rich is installed.\n",
    "\n",
    "import seaborn as sns  # isort: skip  # After a2rl, sns's suprious deprecation warnings are gone.\n",
    "\n",
    "# Misc. settings\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "RAN_SEED = 42\n",
    "random.seed(RAN_SEED)\n",
    "np.random.seed(RAN_SEED)\n",
    "_ = torch.manual_seed(RAN_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE_ROW = 2  # block_size (measured by # of rows) as the context to train GPT\n",
    "wi_df = wi.read_csv_dataset(wi.sample_dataset_path(\"chiller\"))\n",
    "wi_df.add_value()\n",
    "\n",
    "# Speed up training for demo purpose\n",
    "wi_df = wi_df.iloc[:1000]\n",
    "\n",
    "# Instantiate a tokenier given the selected dataset.\n",
    "tokenizer = wi.AutoTokenizer(wi_df, block_size_row=BLOCK_SIZE_ROW)\n",
    "tokenizer.df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.df_tokenized.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load or Train the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model-byo\"\n",
    "config = None  # Default training configuration\n",
    "\n",
    "################################################################################\n",
    "# To run in fast mode, set env var NOTEBOOK_FAST_RUN=1 prior to starting Jupyter\n",
    "################################################################################\n",
    "if os.environ.get(\"NOTEBOOK_FAST_RUN\", \"0\") != \"0\":\n",
    "    config = {\n",
    "        \"train_config\": {\n",
    "            \"epochs\": 1,\n",
    "            \"batch_size\": 512,\n",
    "            \"embedding_dim\": 512,\n",
    "            \"gpt_n_layer\": 1,\n",
    "            \"gpt_n_head\": 1,\n",
    "            \"learning_rate\": 6e-4,\n",
    "            \"num_workers\": 0,\n",
    "            \"lr_decay\": True,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<p style=\"color:firebrick; background-color:yellow; font-weight:bold\">'\n",
    "            \"NOTE: notebook runs in fast mode. Use only 1 epoch. Results may differ.\"\n",
    "        )\n",
    "    )\n",
    "################################################################################\n",
    "\n",
    "builder = wi.GPTBuilder(tokenizer, model_dir, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start GPT model training.\n",
    "\n",
    "Default hyperparam is located at `src/a2rl/config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "model_fname = os.path.join(model_dir, builder.model_name)\n",
    "if os.path.exists(model_fname):\n",
    "    print(f\"Will load the GPT model from {model_fname}\")\n",
    "    builder.load_model()\n",
    "else:\n",
    "    print(\"Training the GPT model\")\n",
    "    builder.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Simulator Instance\n",
    "To create a simulator, we need pass in the tokenzier and the GPT model wrapped inside `a2rl.Simulator.GPTBuilder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = wi.Simulator(tokenizer, builder.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "First we will find out the column names of SARS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = tokenizer.df.sar_d\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_cols = tokenizer.df.sar_d[\"rewards\"]\n",
    "rewards_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_cols = tokenizer.df.sar_d[\"actions\"]\n",
    "action_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = len(tokenizer.df_tokenized[action_cols[0]].unique())\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total number of dataframe tokens per SAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_row_len = tokenizer.state_dim  + tokenizer.reward_dim + tokenizer.action_dim\n",
    "sar_row_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = BLOCK_SIZE_ROW * sar_row_len\n",
    "block_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioural Clone Planner\n",
    "First we use the `a2rl.Simulator.sample` API to obtain some random actions.\n",
    "By \"random\", we mean the actions (behaviour) are sampled from the probability distribution learned by the GPT model from the CSV dataset. It is expected these actions are similar to actions reflected in the data, hence the \"clone\" part.\n",
    "We then apply that random actions to rollout the next step.\n",
    "We do this Rollout for each step throughout the entire trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 20  # set the planning horizon to 20 steps\n",
    "nb_runs = 5\n",
    "step_size = tokenizer.state_dim + tokenizer.action_dim + tokenizer.reward_dim\n",
    "################################################################################\n",
    "# To run in fast mode, set env var NOTEBOOK_FAST_RUN=1 prior to starting Jupyter\n",
    "################################################################################\n",
    "if os.environ.get(\"NOTEBOOK_FAST_RUN\", \"0\") != \"0\":\n",
    "    nb_runs = 2\n",
    "\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<p style=\"color:firebrick; background-color:yellow; font-weight:bold\">'\n",
    "            \"NOTE: notebook runs in fast mode. Use less samples. Results may differ.\"\n",
    "        )\n",
    "    )\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accum_cost_list = []\n",
    "non_accum_cost_list = []\n",
    "batch_size = nb_runs\n",
    "custom_context = tokenizer.df_tokenized.iloc[0, :tokenizer.state_dim].values\n",
    "per_ctx_max_size = 1\n",
    "# obtain a valid \"random\" action\n",
    "for i in tqdm(range(horizon)):\n",
    "    if custom_context.ndim == 1:\n",
    "        batch_custom_context = np.tile(custom_context, (batch_size, 1))\n",
    "    else:\n",
    "        batch_custom_context = custom_context\n",
    "    \n",
    "    if batch_custom_context.shape[1] > block_size:\n",
    "        truncated_custom_context = batch_custom_context[:, -block_size:]\n",
    "    else:\n",
    "        truncated_custom_context = batch_custom_context\n",
    "    \n",
    "    recommendation_df = simulator.sample(\n",
    "        truncated_custom_context, max_size=per_ctx_max_size, as_token=True\n",
    "    )\n",
    "    my_actions = recommendation_df[action_cols].values\n",
    "    reward, next_states = simulator.lookahead(batch_custom_context, my_actions)\n",
    "    # only keep the first reward/state that corresponds to the \n",
    "    # matching (which happens to be the first) action of a given custom_context\n",
    "    reward = reward[::batch_size]\n",
    "    next_states = next_states[::batch_size]\n",
    "    \n",
    "    samples = np.hstack([my_actions, reward, next_states])\n",
    "\n",
    "    df_ars = wi.WiDataFrame(\n",
    "        samples,\n",
    "        **tokenizer.df_tokenized.sar_d,\n",
    "        columns=[\n",
    "            *tokenizer.df_tokenized.actions,\n",
    "            *tokenizer.df_tokenized.rewards,\n",
    "            *tokenizer.df_tokenized.states,\n",
    "        ],\n",
    "    )\n",
    "    df_sar = df_ars[df_ars.sar]\n",
    "    df_sar = tokenizer.field_tokenizer.inverse_transform(df_sar)\n",
    "    immediate_cost = df_sar[tokenizer.df_tokenized.rewards[:-1]].values\n",
    "    custom_context = np.hstack([batch_custom_context, samples])\n",
    "    non_accum_cost_list.append(immediate_cost.flatten().tolist())\n",
    "\n",
    "accum_cost_list = np.array(non_accum_cost_list)\n",
    "non_accum_cost_list = np.array(non_accum_cost_list)\n",
    "for i in range(1, len(non_accum_cost_list)):\n",
    "    accum_cost_list[i, :] = accum_cost_list[i - 1, :] + non_accum_cost_list[i, :]\n",
    "\n",
    "accum_cost_list = accum_cost_list.transpose()\n",
    "non_accum_cost_list = non_accum_cost_list.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-value Maximisation Planner\n",
    "Second we use the `a2rl.Simulator.get_valid_actions` to obtain all the valid actions.\n",
    "Then we use `a2rl.Simulator.lookahead` to \"explore\" each action by obtaining both immediate reward and reward-to-go.\n",
    "Next, we choose the action that has the highest / lowest sum_reward (immediate_reward + reward-to-go), and take that action to the next step. We do this Rollout for the entire trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_accum_cost_list = []\n",
    "q_non_accum_cost_list = []\n",
    "\n",
    "batch_size = nb_runs\n",
    "custom_context = tokenizer.df_tokenized.iloc[0, : tokenizer.state_dim].values\n",
    "\n",
    "for i in tqdm(range(horizon)):\n",
    "    if custom_context.ndim == 1:\n",
    "        batch_custom_context = np.tile(custom_context, (batch_size, 1))\n",
    "    else:\n",
    "        batch_custom_context = custom_context\n",
    "\n",
    "    if batch_custom_context.shape[1] > block_size:\n",
    "        truncated_custom_context = batch_custom_context[:, -block_size:]\n",
    "    else:\n",
    "        truncated_custom_context = batch_custom_context\n",
    "\n",
    "    all_valid_actions = simulator.get_valid_actions(\n",
    "        truncated_custom_context[0], max_size=nb_actions\n",
    "    ).values\n",
    "\n",
    "    reward, next_states = simulator.lookahead(batch_custom_context, all_valid_actions)\n",
    "\n",
    "    tiled_actions = np.tile(all_valid_actions, (batch_size, 1))\n",
    "    samples = np.hstack([tiled_actions, reward, next_states])\n",
    "    df_ars = wi.WiDataFrame(\n",
    "        samples,\n",
    "        **tokenizer.df_tokenized.sar_d,\n",
    "        columns=[\n",
    "            *tokenizer.df_tokenized.actions,\n",
    "            *tokenizer.df_tokenized.rewards,\n",
    "            *tokenizer.df_tokenized.states,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_sar = df_ars[df_ars.sar]\n",
    "    df_sar = tokenizer.field_tokenizer.inverse_transform(df_sar) # need untokenized (original) reward values\n",
    "    reward = df_sar[tokenizer.df_tokenized.rewards].values\n",
    "\n",
    "    both_cost = reward.sum(axis=1)  # sum of immediate cost + cost_to_go\n",
    "    both_cost = both_cost.reshape([batch_size, -1])\n",
    "    action_idx = np.argmin(both_cost, axis=1)  # for each run gets its min-cost index\n",
    "    rs_reward = reward.reshape([batch_size, -1, tokenizer.reward_dim]) # [nb_runs, nb_actions, reward_dim]\n",
    "    # pick the reward as per the min-cost action\n",
    "    sel_reward = np.array(\n",
    "        [data[action] for data, action in zip(rs_reward, action_idx)]\n",
    "    )    \n",
    "    immediate_cost = sel_reward[:, 0]\n",
    "    q_non_accum_cost_list.append(immediate_cost.flatten().tolist())\n",
    "    \n",
    "    # use the tokenized dataframe to select the new context as per the min-cost action\n",
    "    df_ars_reshape = df_ars.values.reshape([batch_size, -1, len(df_ars.columns)])\n",
    "    new_context = np.array(\n",
    "        [data[action] for idx, (data, action) in enumerate(zip(df_ars_reshape, action_idx))]\n",
    "    )\n",
    "    custom_context = np.hstack([batch_custom_context, new_context])\n",
    "\n",
    "q_accum_cost_list = np.array(q_non_accum_cost_list)\n",
    "q_non_accum_cost_list = np.array(q_non_accum_cost_list)\n",
    "for i in range(1, len(q_non_accum_cost_list)):\n",
    "    q_accum_cost_list[i, :] = q_accum_cost_list[i - 1, :] + q_non_accum_cost_list[i, :]\n",
    "\n",
    "q_accum_cost_list = q_accum_cost_list.transpose()\n",
    "q_non_accum_cost_list = q_non_accum_cost_list.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the costs (`system_power_consumption`) between two planners\n",
    "\n",
    "On average (in the sense of **expected** outcome), the `Q-value Maximisation` planner produces relatively lower `system_power_consumption`. However, the `Bahaviour Clone` actions may occasionally perform equally well. This is due to the non-deterministic nature of both the *Simulator* when performing `simulator.lookahead()` and the randomness associated with `simulator.sample()`. Moreover, the GPT model associated with the *Simulator* in this example was not trained sufficiently in terms of both the number of epochs and the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_list = []\n",
    "policy_list = []\n",
    "acc_cost = []\n",
    "inst_cost = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(nb_runs):\n",
    "    for j in range(horizon):\n",
    "        step_list.append(j)\n",
    "        acc_cost.append(accum_cost_list[i][j])\n",
    "        inst_cost.append(non_accum_cost_list[i][j])\n",
    "        policy_list.append(\"behaviour\")\n",
    "\n",
    "        step_list.append(j)\n",
    "        acc_cost.append(q_accum_cost_list[i][j])\n",
    "        inst_cost.append(q_non_accum_cost_list[i][j])\n",
    "        policy_list.append(\"q-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(\n",
    "    {\n",
    "        \"step\": step_list,\n",
    "        \"acc_cost\": acc_cost,\n",
    "        \"step_cost\": inst_cost,\n",
    "        \"policy\": policy_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=df_result[df_result.policy == \"behaviour\"], x=\"step\", y=\"step_cost\", label=\"Behaviour clone\"\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_result[df_result.policy == \"q-value\"], x=\"step\", y=\"step_cost\", label=\"Q-value optimal\"\n",
    ")\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(ls=\"--\")\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.ylabel(\"Step Cost\", fontsize=16)\n",
    "_ = plt.title(\"Stepwise system_power_consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1 = df_result[(df_result.policy == \"behaviour\")]\n",
    "data2 = df_result[(df_result.policy == \"q-value\")]\n",
    "\n",
    "sns.lineplot(data=data1, x=\"step\", y=\"acc_cost\", label=\"Behaviour clone\")\n",
    "sns.lineplot(data=data2, x=\"step\", y=\"acc_cost\", label=\"Q-value optimal\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(ls=\"--\")\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.ylabel(\"Accumutive Cost\", fontsize=16)\n",
    "_ = plt.title(\"Accumulative system_power_consumption\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
