{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYO Planner\n",
    "\n",
    "This example shows you how to **Bring Your Own** (BYO) planner to work with the `a2rl.Simulator` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import my_nb_path  # isort: skip\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import a2rl as wi\n",
    "from a2rl.nbtools import print  # Enable color outputs when rich is installed.\n",
    "\n",
    "import seaborn as sns  # isort: skip  # After a2rl, sns's suprious deprecation warnings are gone.\n",
    "\n",
    "# Misc. settings\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "RAN_SEED = 42\n",
    "random.seed(RAN_SEED)\n",
    "np.random.seed(RAN_SEED)\n",
    "_ = torch.manual_seed(RAN_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE_ROW = 2  # block_size (measured by # of rows) as the context to train GPT\n",
    "wi_df = wi.read_csv_dataset(wi.sample_dataset_path(\"chiller\"))\n",
    "wi_df.add_value()\n",
    "\n",
    "# Speed up training for demo purpose\n",
    "wi_df = wi_df.iloc[:1000]\n",
    "\n",
    "# Instantiate a tokenier given the selected dataset.\n",
    "tokenizer = wi.AutoTokenizer(wi_df, block_size_row=BLOCK_SIZE_ROW)\n",
    "tokenizer.df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.df_tokenized.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Simulator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load or Train the GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model-byo\"\n",
    "config = None  # Default training configuration\n",
    "\n",
    "################################################################################\n",
    "# To run in fast mode, set env var NOTEBOOK_FAST_RUN=1 prior to starting Jupyter\n",
    "################################################################################\n",
    "if os.environ.get(\"NOTEBOOK_FAST_RUN\", \"0\") != \"0\":\n",
    "    config = {\n",
    "        \"train_config\": {\n",
    "            \"epochs\": 1,\n",
    "            \"batch_size\": 512,\n",
    "            \"embedding_dim\": 512,\n",
    "            \"gpt_n_layer\": 1,\n",
    "            \"gpt_n_head\": 1,\n",
    "            \"learning_rate\": 6e-4,\n",
    "            \"num_workers\": 0,\n",
    "            \"lr_decay\": True,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<p style=\"color:firebrick; background-color:yellow; font-weight:bold\">'\n",
    "            \"NOTE: notebook runs in fast mode. Use only 1 epoch. Results may differ.\"\n",
    "        )\n",
    "    )\n",
    "################################################################################\n",
    "\n",
    "builder = wi.GPTBuilder(tokenizer, model_dir, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start GPT model training.\n",
    "\n",
    "Default hyperparam is located at `src/a2rl/config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "model_fname = os.path.join(model_dir, builder.model_name)\n",
    "if os.path.exists(model_fname):\n",
    "    print(f\"Will load the GPT model from {model_fname}\")\n",
    "    builder.load_model()\n",
    "else:\n",
    "    print(\"Training the GPT model\")\n",
    "    builder.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Simulator Instance\n",
    "To create a simulator, we need pass in the tokenzier and the GPT model wrapped inside `a2rl.Simulator.GPTBuilder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = wi.Simulator(tokenizer, builder.model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Planner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for planners to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 20  # set the planning horizon to 20 steps\n",
    "nb_runs = 5\n",
    "step_size = tokenizer.state_dim + tokenizer.action_dim + tokenizer.reward_dim\n",
    "################################################################################\n",
    "# To run in fast mode, set env var NOTEBOOK_FAST_RUN=1 prior to starting Jupyter\n",
    "################################################################################\n",
    "if os.environ.get(\"NOTEBOOK_FAST_RUN\", \"0\") != \"0\":\n",
    "    nb_runs = 2\n",
    "\n",
    "    display(\n",
    "        Markdown(\n",
    "            '<p style=\"color:firebrick; background-color:yellow; font-weight:bold\">'\n",
    "            \"NOTE: notebook runs in fast mode. Use less samples. Results may differ.\"\n",
    "        )\n",
    "    )\n",
    "################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define three planners\n",
    "\n",
    "Here we define three planner classes as examples to illustrate how to Bring Your Own planner to work with the `A2RL` simulator.\n",
    "<!-- We will add more planners (e.g. `BeamSearchPlanner`, etc.) as needed as per your feedback. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import a2rl\n",
    "from a2rl import Simulator\n",
    "\n",
    "\n",
    "class A2RLPLanner(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        simulator: Simulator,\n",
    "    ) -> None:\n",
    "        self.simulator = simulator\n",
    "        self.tokenizer = simulator.tokenizer\n",
    "        self.block_size = self.tokenizer.block_size\n",
    "        self.action_cols = self.tokenizer.action_columns\n",
    "        self.nb_actions = len(self.tokenizer.df_tokenized[self.action_cols[0]].unique())\n",
    "\n",
    "    def rollout(self, horizon: int = 20, nb_runs: int = 3) -> List[np.array]:\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            the first element of the list is a [nb_runs, horizon] array\n",
    "                with immeidate cost/reward for run M (0 <= M < nb_runs), step N (0 <= N < horizon)\n",
    "            the second element of the list is a [nb_runs, horizon] array\n",
    "                with accumulative cost/reward for run M (0 <= M < nb_runs), step N (0 <= N < horizon)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"A2RLPLanner needs to be subclassed\")\n",
    "\n",
    "\n",
    "class BehaviourClonePlanner(A2RLPLanner):\n",
    "    \"\"\"\n",
    "    we use the `a2rl.Simulator.sample` API to obtain some random actions.\n",
    "    By \"random\", we mean the actions (behaviour) are sampled from the probability distribution\n",
    "    learned by the GPT model from the CSV dataset.\n",
    "    It is expected these actions are similar to actions reflected in the data, hence the \"clone\" part.\n",
    "    We then apply that random actions to rollout the next step.\n",
    "    We do this Rollout for each step throughout the entire trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, simulator: Simulator) -> None:\n",
    "        super().__init__(simulator)\n",
    "\n",
    "    def _get_valid_action_indices(self, batch_size, nb_total_actions):\n",
    "        \"\"\"\n",
    "        Each run or context in a batch corresponds to (nb_total_actions // batch_size) actions\n",
    "        e.g. if `contexts` has 2 runs, calling simulator.sample(contexts, max_size=3) will\n",
    "        return 6 actions, each run/context has 3 actions, with the first 3 stacked atop the last 3.\n",
    "\n",
    "        Now calling lookahead will produce 2 * 6 = 12 context-action pairs (cartesian products).\n",
    "        However, only the first three ([0, 1, 2]) and the last three pairs ([9, 10, 11]) are valid pairs,\n",
    "        i.e. pairs where a context matches its own sampled acitons.\n",
    "\n",
    "        This function returns indices for all valid pairs.\n",
    "        \"\"\"\n",
    "        if nb_total_actions % batch_size != 0:\n",
    "            raise Exception(\"call simulator.sample() first\")\n",
    "        act_per_ctx = nb_total_actions // batch_size\n",
    "        indices = []\n",
    "        # actions_per_context = nb_total_actions // batch_size\n",
    "        for i in range(batch_size):\n",
    "            start = (i * batch_size + i) * act_per_ctx\n",
    "            end = start + act_per_ctx\n",
    "            indices.append((start, end))\n",
    "        return indices\n",
    "\n",
    "    def rollout(self, horizon: int = 20, nb_runs: int = 3) -> List[np.array]:\n",
    "        \"\"\"\n",
    "        rollout trajectories using actions learned from the past data\n",
    "        \"\"\"\n",
    "        accum_cost_list = []  # shape: [nb_horizon, nb_runs]\n",
    "        non_accum_cost_list = []  # shape: [nb_horizon, nb_runs]\n",
    "        batch_size = nb_runs\n",
    "        custom_context = self.tokenizer.df_tokenized.iloc[0, : self.tokenizer.state_dim].values\n",
    "        per_ctx_max_size = 1\n",
    "        # obtain a valid \"random\" action\n",
    "        for i in tqdm(range(horizon)):\n",
    "            if custom_context.ndim == 1:\n",
    "                batch_custom_context = np.tile(custom_context, (batch_size, 1))\n",
    "            else:\n",
    "                batch_custom_context = custom_context\n",
    "\n",
    "            if batch_custom_context.shape[1] > self.block_size:\n",
    "                truncated_custom_context = batch_custom_context[:, -self.block_size :]\n",
    "            else:\n",
    "                truncated_custom_context = batch_custom_context\n",
    "\n",
    "            recommendation_df = self.simulator.sample(\n",
    "                truncated_custom_context, max_size=per_ctx_max_size, as_token=True\n",
    "            )\n",
    "            my_actions = recommendation_df[self.action_cols].values\n",
    "            reward, next_states = self.simulator.lookahead(batch_custom_context, my_actions)\n",
    "\n",
    "            valid_indices = self._get_valid_action_indices(batch_size, my_actions.shape[0])\n",
    "            reward_sl, next_states_sl = [], []\n",
    "            for sti, edi in valid_indices:\n",
    "                reward_sl.append(reward[sti:edi])\n",
    "                next_states_sl.append(next_states[sti:edi])\n",
    "            # pick the reward and states as per their matching actions wrt. contexts\n",
    "            reward = np.vstack(reward_sl)\n",
    "            next_states = np.vstack(next_states_sl)\n",
    "\n",
    "            samples = np.hstack([my_actions, reward, next_states])\n",
    "\n",
    "            df_ars = a2rl.WiDataFrame(\n",
    "                samples,\n",
    "                **self.tokenizer.df_tokenized.sar_d,\n",
    "                columns=[\n",
    "                    *self.tokenizer.df_tokenized.actions,\n",
    "                    *self.tokenizer.df_tokenized.rewards,\n",
    "                    *self.tokenizer.df_tokenized.states,\n",
    "                ],\n",
    "            )\n",
    "            df_sar = df_ars[df_ars.sar]\n",
    "            df_sar = self.tokenizer.field_tokenizer.inverse_transform(df_sar)\n",
    "            immediate_cost = df_sar[self.tokenizer.df_tokenized.rewards[:-1]].values\n",
    "            custom_context = np.hstack([batch_custom_context, samples])\n",
    "            non_accum_cost_list.append(immediate_cost.flatten().tolist())\n",
    "\n",
    "        accum_cost_list = np.array(non_accum_cost_list)\n",
    "        non_accum_cost_list = np.array(non_accum_cost_list)\n",
    "        for i in range(1, len(non_accum_cost_list)):\n",
    "            accum_cost_list[i, :] = accum_cost_list[i - 1, :] + non_accum_cost_list[i, :]\n",
    "\n",
    "        accum_cost_list = accum_cost_list.transpose()\n",
    "        non_accum_cost_list = non_accum_cost_list.transpose()\n",
    "        return [non_accum_cost_list, accum_cost_list]\n",
    "\n",
    "\n",
    "class QPlanner(A2RLPLanner):\n",
    "    \"\"\"\n",
    "    we use the `a2rl.Simulator.get_valid_actions` to obtain all the valid actions.\n",
    "    Then we use `a2rl.Simulator.lookahead` to \"explore\" each action by obtaining\n",
    "    both immediate reward and reward-to-go.\n",
    "    Next, we choose the action that has the highest / lowest sum_reward (immediate_reward + reward-to-go),\n",
    "    and take that action to the next step. We do this Rollout for the entire trajectory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, simulator: Simulator, objective: str = \"min\") -> None:\n",
    "        super().__init__(simulator)\n",
    "        if objective.lower() not in [\"min\", \"max\"]:\n",
    "            raise ValueError(\"objective must be either min or max\")\n",
    "        if \"min\" == objective:\n",
    "            self.obj_op = np.argmin\n",
    "        else:\n",
    "            self.obj_op = np.argmax\n",
    "\n",
    "    def rollout(self, horizon: int = 20, nb_runs: int = 3) -> List[np.array]:\n",
    "        q_accum_cost_list = []\n",
    "        q_non_accum_cost_list = []\n",
    "\n",
    "        batch_size = nb_runs\n",
    "        custom_context = self.tokenizer.df_tokenized.iloc[0, : self.tokenizer.state_dim].values\n",
    "\n",
    "        for i in tqdm(range(horizon)):\n",
    "            if custom_context.ndim == 1:\n",
    "                batch_custom_context = np.tile(custom_context, (batch_size, 1))\n",
    "            else:\n",
    "                batch_custom_context = custom_context\n",
    "\n",
    "            if batch_custom_context.shape[1] > self.block_size:\n",
    "                truncated_custom_context = batch_custom_context[:, -self.block_size :]\n",
    "            else:\n",
    "                truncated_custom_context = batch_custom_context\n",
    "\n",
    "            all_valid_actions = self.simulator.get_valid_actions(\n",
    "                truncated_custom_context[0], max_size=self.nb_actions\n",
    "            ).values\n",
    "\n",
    "            reward, next_states = self.simulator.lookahead(batch_custom_context, all_valid_actions)\n",
    "\n",
    "            tiled_actions = np.tile(all_valid_actions, (batch_size, 1))\n",
    "            samples = np.hstack([tiled_actions, reward, next_states])\n",
    "            df_ars = a2rl.WiDataFrame(\n",
    "                samples,\n",
    "                **self.tokenizer.df_tokenized.sar_d,\n",
    "                columns=[\n",
    "                    *self.tokenizer.df_tokenized.actions,\n",
    "                    *self.tokenizer.df_tokenized.rewards,\n",
    "                    *self.tokenizer.df_tokenized.states,\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            df_sar = df_ars[df_ars.sar]\n",
    "            df_sar = self.tokenizer.field_tokenizer.inverse_transform(\n",
    "                df_sar\n",
    "            )  # need untokenized (original) reward values\n",
    "            reward = df_sar[self.tokenizer.df_tokenized.rewards].values\n",
    "\n",
    "            both_cost = reward.sum(axis=1)  # sum of immediate cost + cost_to_go\n",
    "            both_cost = both_cost.reshape([batch_size, -1])\n",
    "            action_idx = self.obj_op(both_cost, axis=1)  # for each run gets its min-cost index\n",
    "            rs_reward = reward.reshape(\n",
    "                [batch_size, -1, self.tokenizer.reward_dim]\n",
    "            )  # [nb_runs, nb_actions, reward_dim]\n",
    "            # pick the reward as per the min-cost action\n",
    "            sel_reward = np.array([data[action] for data, action in zip(rs_reward, action_idx)])\n",
    "            immediate_cost = sel_reward[:, 0]\n",
    "            q_non_accum_cost_list.append(immediate_cost.flatten().tolist())\n",
    "\n",
    "            # use the tokenized dataframe to select the new context as per the min-cost action\n",
    "            df_ars_reshape = df_ars.values.reshape([batch_size, -1, len(df_ars.columns)])\n",
    "            new_context = np.array(\n",
    "                [data[action] for idx, (data, action) in enumerate(zip(df_ars_reshape, action_idx))]\n",
    "            )\n",
    "            custom_context = np.hstack([batch_custom_context, new_context])\n",
    "\n",
    "        q_accum_cost_list = np.array(q_non_accum_cost_list)\n",
    "        q_non_accum_cost_list = np.array(q_non_accum_cost_list)\n",
    "        for i in range(1, len(q_non_accum_cost_list)):\n",
    "            q_accum_cost_list[i, :] = q_accum_cost_list[i - 1, :] + q_non_accum_cost_list[i, :]\n",
    "\n",
    "        q_accum_cost_list = q_accum_cost_list.transpose()\n",
    "        q_non_accum_cost_list = q_non_accum_cost_list.transpose()\n",
    "        return [q_non_accum_cost_list, q_accum_cost_list]\n",
    "\n",
    "\n",
    "class BeamSearchQPlanner(A2RLPLanner):\n",
    "    \"\"\"\n",
    "    This planner has similar logic to the QPlanner, only it uses `a2rl.Simulator.beam_search_n_steps`\n",
    "    to obtain all the actions and rewards in one go.\n",
    "    The actions are still chosen with the highest / lowest sum_reward (immediate_reward + reward-to-go),\n",
    "    and take that action to the next step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, simulator: Simulator, beam_width: int, beam_random: bool, objective: str = \"min\"\n",
    "    ) -> None:\n",
    "        super().__init__(simulator)\n",
    "\n",
    "        self.beam_width = beam_width\n",
    "        self.beam_random = beam_random\n",
    "\n",
    "        if objective.lower() not in [\"min\", \"max\"]:\n",
    "            raise ValueError(\"objective must be either min or max\")\n",
    "        if \"min\" == objective:\n",
    "            self.obj_op = np.argmin\n",
    "        else:\n",
    "            self.obj_op = np.argmax\n",
    "\n",
    "    def rollout(self, horizon: int = 20, nb_runs: int = 3) -> List[np.array]:\n",
    "        if nb_runs != 1:\n",
    "            print(\n",
    "                \"WARN: multiple runs in beam search is implemented as a loop and not vectorized and performance may be slow\"\n",
    "            )\n",
    "\n",
    "        if nb_runs != 1 and not self.beam_random:\n",
    "            raise ValueError(\"'beam_random' should be True when using multiple runs\")\n",
    "\n",
    "        dataframe_per_run = []\n",
    "        non_accum_rewards_list = []\n",
    "        accum_rewards_list = []\n",
    "\n",
    "        initial_context = self.tokenizer.df_tokenized.iloc[0, : self.tokenizer.state_dim].values\n",
    "\n",
    "        for i_run in range(nb_runs):\n",
    "            non_accum_rewards = []\n",
    "\n",
    "            if initial_context.ndim != 1:\n",
    "                raise NotImplementedError(\"batching not implemented\")\n",
    "\n",
    "            # Overwite some tokens if you need. See below \"Create and run the BeamSearchQPlanner\" for details\n",
    "            overwrite_valid_tokens = {}\n",
    "\n",
    "            # Generate A+R+S tokens each time\n",
    "            context = initial_context\n",
    "            n_steps = (\n",
    "                self.tokenizer.action_dim + self.tokenizer.reward_dim + self.tokenizer.state_dim\n",
    "            )\n",
    "\n",
    "            for i in tqdm(range(horizon)):\n",
    "                new_context, accum_logprobs = self.simulator.beam_search_n_steps(\n",
    "                    seq=context,\n",
    "                    n_steps=n_steps,\n",
    "                    beam_width=self.beam_width,\n",
    "                    randomness=self.beam_random,\n",
    "                    overwrite_valid_tokens=overwrite_valid_tokens,\n",
    "                    return_logprobs=True,\n",
    "                )\n",
    "\n",
    "                ars_tokens = new_context[:, len(context) :]\n",
    "                df_ars = wi.WiDataFrame(\n",
    "                    ars_tokens,\n",
    "                    **self.tokenizer.df_tokenized.sar_d,\n",
    "                    columns=[\n",
    "                        *self.tokenizer.action_columns,\n",
    "                        *self.tokenizer.reward_columns,\n",
    "                        *self.tokenizer.state_columns,\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                df_sar = df_ars[df_ars.sar]\n",
    "                df_sar = self.tokenizer.field_tokenizer.inverse_transform(df_sar)\n",
    "\n",
    "                rewards = df_sar[self.tokenizer.reward_columns].values\n",
    "                best_idx = self.obj_op(rewards.sum(axis=1))\n",
    "                non_accum_rewards.append(rewards[best_idx, 0])\n",
    "\n",
    "                context = new_context[best_idx]\n",
    "\n",
    "            # Uncomment the following if you want to record a dataframe per run\n",
    "            # widf_searched = wi.WiDataFrame(\n",
    "            #     context[len(initial_context) :].reshape(horizon, -1),\n",
    "            #     **self.tokenizer.df_tokenized.sar_d,\n",
    "            #     columns=[\n",
    "            #         *self.tokenizer.df_tokenized.actions,\n",
    "            #         *self.tokenizer.df_tokenized.rewards,\n",
    "            #         *self.tokenizer.df_tokenized.states,\n",
    "            #     ],\n",
    "            # )\n",
    "            # widf_searched = widf_searched[widf_searched.sar]\n",
    "            # widf_searched = self.tokenizer.field_tokenizer.inverse_transform(widf_searched)\n",
    "            # widf_searched[\"nb_run\"] = i_run\n",
    "            # widf_searched[\"timestep\"] = range(1, len(widf_searched) + 1)\n",
    "            # dataframe_per_run.append(widf_searched)\n",
    "\n",
    "            non_accum_rewards = np.array(non_accum_rewards)\n",
    "            accum_rewards = np.cumsum(non_accum_rewards, axis=0)\n",
    "\n",
    "            non_accum_rewards_list.append(non_accum_rewards)\n",
    "            accum_rewards_list.append(accum_rewards)\n",
    "\n",
    "        non_accum_rewards_list = np.array(non_accum_rewards_list)\n",
    "        accum_rewards_list = np.array(accum_rewards_list)\n",
    "\n",
    "        return [non_accum_rewards_list, accum_rewards_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run the `BehaviourClonePlanner` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcp = BehaviourClonePlanner(simulator)\n",
    "non_accum_cost_list, accum_cost_list = bcp.rollout(horizon, nb_runs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run the `QPlanner` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QPlanner(simulator)\n",
    "q_non_accum_cost_list, q_accum_cost_list = qp.rollout(horizon, nb_runs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run the `BeamSearchQPlanner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsqp = BeamSearchQPlanner(simulator, beam_width=8, beam_random=True)\n",
    "bsq_non_accum_cost_list, bsq_accum_cost_list = bsqp.rollout(horizon, nb_runs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use the `overwrite_valid_tokens` argument in Beam Search\n",
    "\n",
    "Beam Search has another feature to constrain the tokens sampled in a column, via the\n",
    "`overwrite_valid_tokens` argument in `beam_search_n_steps`. This feature is useful if\n",
    "additional constraint needs to imposed during inference that is not present in the\n",
    "training data.\n",
    "\n",
    "For example:\n",
    "- Column \"Action_1\" contains 5 valid actions: **[101, 102, 103, 104, 105]**. During\n",
    "training, all 5 actions are present in the training data, so the simulator learnt to use\n",
    "all 5 actions.\n",
    "- However, during inference, due to external constraint, only 3 actions are valid:\n",
    "**[101, 103, 105]**, and we still want to see which action is the best, so we use the\n",
    "`overwrite_valid_tokens` argument.\n",
    "```python\n",
    "overwrite_valid_tokens = {\"Action_1\": [101, 103, 105]}\n",
    "```\n",
    "- If there's another column with similar constraint:\n",
    "```python\n",
    "overwrite_valid_tokens = {\"Action_1\": [101, 103, 105], \"State_3\": [203, 204]}\n",
    "```\n",
    "- A special case is if you want to fix a column during sampling:\n",
    "```python\n",
    "overwrite_valid_tokens = {\"Action_1\": [101]}\n",
    "```\n",
    "\n",
    "Depending on where you create and use this argument, you can dynamically constrain part\n",
    "of the rollout:\n",
    "```python\n",
    "def rollout():\n",
    "    for timestep in range(horizon):\n",
    "        if timestep < 10:\n",
    "            overwrite_valid_tokens = {\"Action_1\": [101, 103, 105]}\n",
    "        else:\n",
    "            overwrite_valid_tokens = None  # no overwriting\n",
    "\n",
    "        simulator.beam_search_n_steps(...)\n",
    "```\n",
    "or apply during the entire rollout:\n",
    "```python\n",
    "def rollout():\n",
    "    overwrite_valid_tokens = {\"Action_1\": [101, 103, 105]}\n",
    "    for timestep in range(horizon):\n",
    "        simulator.beam_search_n_steps(...)\n",
    "```\n",
    "\n",
    "This overwriting is applied *before* said column is sampled. So that column and all\n",
    "subsequent columns are affected by the constraint.\n",
    "\n",
    "This is also useful if there's a special token such as `<EOS>` which is present in the\n",
    "training data but should not appear in inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the costs (`system_power_consumption`) between three planners\n",
    "\n",
    "On average (in the sense of **expected** outcome), the `Q-value Maximisation` planner (`QPlanner` for short) produces relatively lower `system_power_consumption`. However, the `Bahaviour Clone` actions may occasionally perform equally well. This is due to the non-deterministic nature of both the *Simulator* when performing `simulator.lookahead()` and the randomness associated with `simulator.sample()`. Moreover, the GPT model associated with the *Simulator* in this example was not trained sufficiently in terms of both the number of epochs and the size of the training data.\n",
    "\n",
    "The beam search planner should demonstrate a performance between behaviour cloning and Q-planner, since the idea of beam search is to create a better simulation and ask the planner not to be over-confident about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_list = []\n",
    "policy_list = []\n",
    "acc_cost = []\n",
    "inst_cost = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(nb_runs):\n",
    "    for j in range(horizon):\n",
    "        step_list.append(j)\n",
    "        acc_cost.append(accum_cost_list[i][j])\n",
    "        inst_cost.append(non_accum_cost_list[i][j])\n",
    "        policy_list.append(\"behaviour\")\n",
    "\n",
    "        step_list.append(j)\n",
    "        acc_cost.append(q_accum_cost_list[i][j])\n",
    "        inst_cost.append(q_non_accum_cost_list[i][j])\n",
    "        policy_list.append(\"q-value\")\n",
    "\n",
    "        step_list.append(j)\n",
    "        acc_cost.append(bsq_accum_cost_list[i][j])\n",
    "        inst_cost.append(bsq_non_accum_cost_list[i][j])\n",
    "        policy_list.append(\"beam-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(\n",
    "    {\n",
    "        \"step\": step_list,\n",
    "        \"acc_cost\": acc_cost,\n",
    "        \"step_cost\": inst_cost,\n",
    "        \"policy\": policy_list,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=df_result[df_result.policy == \"behaviour\"],\n",
    "    x=\"step\",\n",
    "    y=\"step_cost\",\n",
    "    label=\"Behaviour clone\",\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_result[df_result.policy == \"q-value\"], x=\"step\", y=\"step_cost\", label=\"Q-value optimal\"\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=df_result[df_result.policy == \"beam-search\"], x=\"step\", y=\"step_cost\", label=\"Beam search\"\n",
    ")\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(ls=\"--\")\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.ylabel(\"Step Cost\", fontsize=16)\n",
    "_ = plt.title(\"Stepwise system_power_consumption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data1 = df_result[(df_result.policy == \"behaviour\")]\n",
    "data2 = df_result[(df_result.policy == \"q-value\")]\n",
    "data3 = df_result[(df_result.policy == \"beam-search\")]\n",
    "\n",
    "sns.lineplot(data=data1, x=\"step\", y=\"acc_cost\", label=\"Behaviour clone\")\n",
    "sns.lineplot(data=data2, x=\"step\", y=\"acc_cost\", label=\"Q-value optimal\")\n",
    "sns.lineplot(data=data3, x=\"step\", y=\"acc_cost\", label=\"Beam search\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(ls=\"--\")\n",
    "plt.xlabel(\"Step\", fontsize=16)\n",
    "plt.ylabel(\"Accumutive Cost\", fontsize=16)\n",
    "_ = plt.title(\"Accumulative system_power_consumption\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
